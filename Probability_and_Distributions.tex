
\section{Probability and Distribution}
\subsection{Random Variables and Distributions}
\begin{definition}
A \textbf{probability space} $(\Omega,\mathfrak{R},\mathbb{P})$ is a triple of a set $\Omega$ and $\sigma$-additive measure $\mathbb{P}$ with domain $\mathfrak{R}$, a $\sigma$-algebra defined on $\Omega$, satisfying $\mathbb{P}(\Omega) = 1$ and $\mathbb{P}(\varnothing)=0$.
\end{definition}
A event can be represented as an element in $\Omega$. A type of events can be abstracted as a subset $A\in\Omega$. The measure $\mathbb{P}(A)$ is called the probability of event $A$ happens. If $\mathbb{P}(A)=1$, we say that $A$ will occurs almost surely.

\begin{definition}
A \textbf{random variable} $X$ on $(\Omega,\mathfrak{R},\mathbb{P})$ is a $\mathfrak{R}$-measurable function $X:\Omega\to\realR$. The $\mathfrak{R}$-measurable here means:
	\begin{equation}
		X^{-1}(B)=\{\omega:\omega\in\Omega, X(\omega)\in B\}\in\mathfrak{R},
	\end{equation}
where $B$ is any Borel subset of $\realR$.
\end{definition}
The concept of random variables is that a function encoding the event into numbers to mathematically modeling the events and trials. Furthermore, the random variable will natrually induce the following conceptation:
\begin{definition}
The \textbf{distribution measure} $\mu_X$ of $X$ is a measure defined as $\mu_X(B)=\mathbb{P}\{X^{-1}(B)\}$, where $B$ is any Borel subset of $\realR$.
\end{definition}
Since the distribution measure $\mu_X$ is defined on $\realR$ and there's a natural measure for $\realR$ as linear measure, the \textbf{Radon-Nikodym}'s theorem implies that there exists a non-negative function $f(x)$ such that the $\mu_X$ can be represented as
\begin{equation}
\mu_X(B) = \int_B f(x)d x, \quad\forall B\in\realR,
\end{equation} 
where the $B$ is a Borel subset of $\realR$. This function $f(X)$ is called the \textbf{probability density function} (PDF). A \textbf{cumulative distribution function}(CDF) $F(x)$ is defined as $F(x)=\mathbb{P}\{X\le x\}$.

\begin{definition} a $\sigma$-algebra generated by a random variable $X$, denoted as $\sigma(X)$ is the collection of subsets $X^{-1}(B)$ where $B$ is any Borel subset of $\realR$. Since $X$ is required to be $\mathfrak{R}$-measurable by definition, it follows $\sigma(X)\subseteq\mathfrak{R}$. Furthermore, if suppose two $\sigma$-algebra $\mathfrak{G,H}\subseteq\mathfrak{R}$, we called them are \textbf{independent} with each other if
	\begin{equation}
	\mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B),\quad \forall A\in\mathfrak{G}, \forall B\in\mathfrak{H}.
	\end{equation}
We say two random varialbes $X$ and $Y$ are independent to each other if $\sigma(X)$ and $\sigma(Y)$ are independent to each other, denoted as $X\ci Y$.
\end{definition}

\begin{definition}
If a random vairable $X$ is integrable, the \textbf{expectation} of $X$, denoted as $\mathbb{E}(X)$ is defined as
	\begin{equation}
	\mathbb{E}(X)=\int_{\Omega}X(\omega) d\mathbb{P}
	\end{equation}
\end{definition}

\begin{definition}
A \textbf{moment generating function} $M_X(t)$, $t\in\realR$ for a random varialbe $X$ is definted as $M_X(t)=\mathbb{E}e^{tX}$.
\end{definition}

\begin{theorem}
The following properties of moment generating functions are straightforward:
	\begin{enumerate}
	\item If $X_i$ are random variables independent to each other and $a_i\in\realR$, then $M_{\sum_ia_iX_i}(t)=\prod_i M_{a_iX_i}(t)$.
	\item If $M_X(t)=M_Y(t)$, then $X=Y$.
	\end{enumerate}
\end{theorem}


\section{Probability and Distribution}
\subsection{Random Variables and Distributions}
\begin{definition}\index{Probability Space}
A \textbf{probability space} $(\Omega,\mathfrak{R},\mathbb{P})$ is a triple of a set $\Omega$ and $\sigma$-additive measure $\mathbb{P}$ with domain $\mathfrak{R}$, a $\sigma$-algebra defined on $\Omega$, satisfying $\mathbb{P}(\Omega) = 1$ and $\mathbb{P}(\varnothing)=0$.
\end{definition}
A event can be represented as an element in $\Omega$. A type of events can be abstracted as a subset $A\in\Omega$. The measure $\mathbb{P}(A)$ is called the probability of event $A$ happens. If $\mathbb{P}(A)=1$, we say that $A$ will occurs almost surely.

\begin{definition}\index{Random variable}
A \textbf{random variable} $X$ on $(\Omega,\mathfrak{R},\mathbb{P})$ is a $\mathfrak{R}$-measurable function $X:\Omega\to\realR$. The $\mathfrak{R}$-measurable here means:
	\begin{equation}
		X^{-1}(B)=\{\omega:\omega\in\Omega, X(\omega)\in B\}\in\mathfrak{R},
	\end{equation}
where $B$ is any Borel subset of $\realR$.
\end{definition}
The concept of random variables is that a function encoding the event into numbers to mathematically modeling the events and trials. Furthermore, the random variable will natrually induce the following conceptation:

\begin{definition}
\index{Random variable!Distribution measure}
The \textbf{distribution measure} $\mu_X$ of $X$ is a pushforward measure induced by $X$ as $\mu_X(B)=X_*\mathbb{P}=\mathbb{P}\{X^{-1}(B)\}$, where $B$ is any Borel subset of $\realR$.
\end{definition}
The \textbf{Radon-Nikodym}'s theorem implies that there exists a non-negative function $f(x)$ bridged the distribution measure $\mu_X$ and the natural linear measure of $\realR$ as
\begin{equation}
\mu_X(B) = \int_B f(x)d x, \quad\forall B\in\realR,
\end{equation} 
where the $B$ is a Borel subset of $\realR$. This function $f(X)$ is called the \textbf{probability density function} (PDF). A \textbf{cumulative distribution function}(CDF) $F(x)$ is defined as $F(x)=\mathbb{P}\{X\le x\}$.
\index{Random variable!Probability distribution function}
\index{Random variable!Accumulative distribution function}
Assum $g$ is a measurable function and $g(x)f(x)$ is integrable, then
\begin{equation}
\int_{\realR} g(x)d\mu_X=\int_{X^{-1}(\realR)}(g\circ X)(\omega)d\mathbb{P}=\int_\realR g(x)f(x)dx.
\end{equation}
To simplify, we assume that $X^{-1}(\realR)=\Omega$ for any random variable defined on $(\Omega,\mathfrak{R},\mathbb{P})$. Furthermore, suppose $G(x)$ is a function of $X$, and $f(x)$ is PDF of $X$, then
\begin{equation}
\int_\Omega G[X(\omega)]d\mathbb{P} = \int_{\realR} G(x)f(x)dx,
\label{eq:expectation_representation}
\end{equation}
if $G(x)f(x)$ is integrable over $\realR$ respect to the natural linear measure.

\begin{definition}
\index{Random variable!Expectation}
If a random vairable $X$ is integrable, the \textbf{expectation} of $X$, denoted as $\mathbb{E}(X)$ is defined as
	\begin{equation}
	\mathbb{E}(X)=\int_{\Omega}X(\omega) d\mathbb{P}.
	\end{equation}
\end{definition}
Based on the Eq.\ref{eq:expectation_representation}, the expectation $\mathbb{E}(X)$ can be calculated from
\begin{equation*}
\mathbb{E}(X)=\int_\realR xf(x)dx.
\end{equation*}

\begin{definition}
\index{Random variable!Moment generating functions}
A \textbf{moment generating function} $M_X(t)$, $t\in\realR$ for a random varialbe $X$ is definted as $M_X(t)=\mathbb{E}e^{tX}$.
\end{definition}

\begin{theorem}
The following properties of moment generating functions are straight forward:
	\begin{enumerate}
	\item If $X_i$ are random variables independent to each other and $a_i\in\realR$, then $M_{\sum_ia_iX_i}(t)=\prod_i M_{a_iX_i}(t)$.
	\item $\mathbb{E}(X^n)=M_X^{(n)}(0)$, $n$th derivative of $M_X(t)$.
	\item If $M_X(t)=M_Y(t)$, then $X=Y$.
	\end{enumerate}
\end{theorem}


\begin{definition}
\index{Random variable!Independence}
A $\sigma$-algebra generated by a random variable $X$, denoted as $\sigma(X)$ is the collection of subsets $X^{-1}(B)$ where $B$ is any Borel subset of $\realR$. Since $X$ is required to be $\mathfrak{R}$-measurable by definition, it follows $\sigma(X)\subseteq\mathfrak{R}$. Furthermore, if suppose two $\sigma$-algebra $\mathfrak{G,H}\subseteq\mathfrak{R}$, we called them are \textbf{independent} with each other if
	\begin{equation}
	\mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B),\quad \forall A\in\mathfrak{G}, \forall B\in\mathfrak{H}.
	\end{equation}
We say two random varialbes $X$ and $Y$ are independent if $\sigma(X)$ and $\sigma(Y)$ are independent, denoted as $X\ci Y$.
\end{definition}



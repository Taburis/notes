
\section{Convention}
\begin{itemize}
\item $n$-tiple $(a_i)_n = (a_1,\dots,a_n)$.
\item $n$ sequence $\{a_i\}_n=a_1,\dots,a_n$.
\item Given to space $V$ and $W$, $V\cong W$ means $V$ is isomorphic to $W$.
\end{itemize}

\section{Multilinear Algebra}

\subsection{Linear Space}
\begin{definition}
A $\mathfrak{F}$-valued \textbf{vector space} $V$ is a set associated with a field $\mathfrak{F}$and two commutable binary mapping (operation), addition $+:V\times V\to V$ and scalar multiplication $\cdot:\mathfrak{F}\times V\to V$, satisfying these conditions
\begin{enumerate}
\item linearity
\begin{equation}
\begin{aligned}
a(\boldsymbol{v}+\boldsymbol{u}) &=a\boldsymbol{u}+a\boldsymbol{v},\\
(a+b)\boldsymbol{v} &= a\boldsymbol{u}+b\boldsymbol{v},\\
a(b\boldsymbol{u}) &= (ab)\boldsymbol{u},\\
0\cdot \boldsymbol{u} &= 0, 1\cdot\boldsymbol{u}=\boldsymbol{u}.
\end{aligned}
\end{equation}
\end{enumerate}
hold $\forall a,b \in \mathfrak{F}$ and $\boldsymbol{u},\boldsymbol{v}\in V$. The element in $V$ is called \textbf{vector} and the element in $\mathfrak{F}$ is called $\textbf{scalar}$. n vectors $\boldsymbol{v}_1,\dots,\boldsymbol{v}_n$ is said to be \textbf{linear independence} if
\begin{equation}
\sum_{i=1}^na_i\boldsymbol{v}_i=0,
\end{equation}
hold only if $a_1=\dots=a_n=0$. The \textbf{dimension} $n$ of a vector space is maximum number $n$ of linear independent vectors, and these linear independent vectors, say $\{\boldsymbol{v}_i\}_n$ formed a \textbf{basis} of this space which allow to express any vector in $V$ as a linear combination of the basis
\begin{equation}
\boldsymbol{u} = \sum_{i=1}^na_i\boldsymbol{v}_i,\quad \forall \boldsymbol{u}\in V,
\end{equation}
we furthre call the $n$-tuple $(a_1,\dots,a_n)$ as the \textbf{coordiate} of the vector $\boldsymbol{u}$ (with respect to this basis).
A mapping $f:V\to W$, where $W$ is a vector space, is called \textbf{linear} if
\begin{equation}
f(a\boldsymbol{u}+b\boldsymbol{v})=af(\boldsymbol{u})+bf(\boldsymbol{v}).
\end{equation}
Further more, a mapping $f:V_1\times\dots\times V_r\to W$ is $r$-\textbf{linear} if the linearity exists for any $i$th argument of $f(\boldsymbol{v}_1,\dots,\boldsymbol{v}_i,\dots,\boldsymbol{v}_n)$. All $r$-linear mapping form a vector space denoted as $\mathcal{L}(V_1,\dots,V_r; W)$. (A 2-linear function is called \textbf{bilinear} sometimes.)

A \textbf{Dual space} of $V$, donoted as $V^*$ is a vector space consistant of all the linear function $f:V\to\mathfrak{F}$. This definition is valid since $f+g$ and $af$ is clearly a linear function as well for any $f,g\in V^*$ and $a\in\mathfrak{a}$.  
\end{definition}

\begin{theorem}
Given vector space $V$ with basis $\{\boldsymbol{v}_i\}_n$ and the dual space $V^*$, if $\dim V=n$ then $\dim V^*=n$. The function $\boldsymbol{v}^{*i}(\boldsymbol{v}_j)=\delta_j^i$ formed a basis of $V^*$. $V^{**}$, the dual space of $V^*$, is isomorphic to $V$. 
\end{theorem}

\begin{proof}
For any linear function $f$ and vector $\boldsymbol{x}=\sum_ix_i\boldsymbol{v}_i$ we have
\begin{equation}
f(\boldsymbol{x})=\sum_{i=1}^nx_if(\boldsymbol{v}_i)=\sum_{i=1}^nf_i\boldsymbol{v}^{*i}(\boldsymbol{x}),
\end{equation}
where $f_i=f(\boldsymbol{v}_i)$. Then $f=\sum_if_i\boldsymbol{v}^{*i}$. This equation also shows that $V^{**}$ is isomorphic to $V$. 
\end{proof}

\begin{definition}
Given a vector space $V$ and $V^*$, define
\begin{equation}
\langle \boldsymbol{v}^*,\boldsymbol{u}\rangle = \boldsymbol{v}^*(\boldsymbol{u}), \quad \forall \boldsymbol{u}\in V, \quad \forall \boldsymbol{v}^*\in V^*.
\end{equation}
This mapping $\langle,\rangle:V^*\times V\to\mathfrak{F}$ is bilinear. Given vector space $V$ and $W$, the direct product $V\times W$ is a vector space as well with dimension $\dim V\cdot\dim W$. A \textbf{tensor product} is a bilinear mapping $\otimes:V\times W\to V\otimes W$, and, $V\times W\cong V\otimes W$. Furthermore, the linear function on $V\otimes W$ can be defined as
\begin{equation}
v^*\otimes w^*(\boldsymbol{x}\otimes\boldsymbol{y})=v^*(\boldsymbol{x})\cdot w^*(\boldsymbol{y}),\quad \forall v^*\in V^*\,\,\text{and}\,\,\forall w* \in W^*,
\end{equation}
Or rewrite it as
\begin{equation}
\langle v^*\otimes w^*,\boldsymbol{x}\otimes\boldsymbol{y}\rangle=\langle v^*,\boldsymbol{x}\rangle\cdot\langle w^*,\boldsymbol{y}\rangle.
\end{equation}
which means that $(V\otimes W)^*=V^*\otimes W^*$.

For the mapping $f$ in $\mathcal{L}(V\otimes W;Z)$, a \textbf{kernel} $\mathcal{K}_f(V\otimes W)$ is a subspace of $V\otimes W$ that $f(x\otimes y)=0,\forall x\otimes y \in V\otimes W$. If $\mathcal{K}_f(V\otimes W)$ is trivial, which means $\mathcal{K}_f(V\otimes W)=0$, then $f\in\mathrm{Hom}(V\otimes W;Z)$, the space of all isomorphism mapping $V\otimes W\to Z$.
\end{definition}

\begin{theorem}
Given vector spaces $V$, $W$ and $Z$, then $V\times W\cong V\otimes W$ and then $\mathcal{L}(V\times W;Z)\cong\mathcal{L}(V\otimes W;Z)$. Tensor product is associative under the product $\langle,\rangle$
\end{theorem}

\subsection{Tensors}
\begin{definition}
Given a vector space $V$ and dual space $V^*$, a element in the space 
\begin{equation}
V^r_s=\underbrace{V\otimes\cdots\otimes V}_{r\text{ terms}}\otimes \underbrace{V^*\otimes\cdots\otimes V^*}_{s\text{ terms}}.
\end{equation}
The basis of $V^r_s$ can be expressed as
\begin{equation}
e_{i_1}\otimes\cdots\otimes e_{i_r}\otimes e^{*k_1}\otimes\cdots\otimes e^{*k_s},
\end{equation}
where $\{e_i\}_{1\le i\le n}$ and $\{e^{*j}\}_{1\le j\le n}$ is the basis of $V$ and $V^*$, respectively. A $V^r_0$ is called a \textbf{contravariant space} and $V_s^0$ is called \textbf{covariant space}.
\index{Tensor!Tensor Space}
\index{Tensor!Contravariant and Covariant}
\end{definition}

The \textbf{Einstein summation rule} in tensor is that omitting the summation sign for the summation over the superscript and the subscript index in the same term
\begin{equation*}
x^{ij}e_{i}=\sum_{i=1}^rx^{ij}e_i.
\end{equation*}
Any tensor $x\in V^r_s$ can be expressed as
\begin{equation}
x=x^{i_1\dots i_r}_{k_1\dots k_s}e_{i_1}\otimes\cdots\otimes e_{i_r}\otimes e^{*k_1}\otimes\cdots\otimes e^{*k_s},
\end{equation}
where the coefficient $x^{i_1\dots i_r}_{k_1\dots k_s}e_{i_1}$ is the cooridnate of $x$ with respect to the basis $\{e_i\}_{1\le i\le n}$ of $V$. And the cooridnate can be expressed a functoin value for $x\in V^r_s$ since any $x$ is a $(r+s)$-linear function on $V^s_r$:
\begin{equation}
x^{i_1\dots i_r}_{k_1\dots k_s}=x(e^{*i_1},\dots, e^{*i_r},e_{k_1},\cdots, e_{k_s}).
\end{equation}

\begin{theorem}
Suppose a linear transformation $V\to V$ that change the coordinate from $e_i\to \bar e_j$, then it can be expressed
\begin{equation}
\bar e_i=\alpha_i^je_j,\quad \bar e^*{k}=(\alpha^{-1})^k_le^{*l},\quad \alpha^i_j(\alpha^{-1})^j_k=\delta^i_k.
\end{equation}
In that case, any $x\in V^r_s$, the coordinate $x^{k_1\dots k_r}_{l_1\dots l_s}$ with respect to basis $\{e_i\}_{1\le i\le n}$ and the coordinate $\bar x^{i_1\dots i_r}_{j_1\dots j_s}$ with respect to the coordinate $\{\bar e_i\}_{1\le i\le n}$ in the different two basis is connected as
\begin{equation}
x^{k_1\dots k_r}_{l_1\dots l_s}=\bar x^{i_1\dots i_r}_{j_1\dots j_s} \alpha_{i_1}^{k_1}\cdots\alpha_{i_r}^{k_r}\cdot (\alpha^{-1})_{l_1}^{j_1}\cdots(\alpha^{-1})_{l_s}^{j_s}.
\end{equation}
If $x$ is a $(r,s)$-type tensor and $y$ is a $(l,h)$-type tensor, then $z=x\otimes y$ is a $(r+l,s+h)$-type tensor with the cooridnate
\begin{equation}
(x\otimes y)^{i_1\dots i_{r+l}}_{j_1\dots j_{s+h}}=x^{i_1\dots i_r}_{j_1\dots j_s}\cdot y^{i_{r+1}\dots i_{r+l}}_{j_{s+1}\dots j_{s+h}}.
\end{equation}
\end{theorem}

\begin{proof}
The proof is pretty straight forward, we picked up a $(1,1)$-type tensor to illustrate the idea. Suppose $x=x^i_je_i\otimes e^{*j}$, and $\bar e_i=\alpha_i^j e_j$, then
\begin{equation*}
\begin{aligned}
x&=\bar x^i_j\bar e_i\otimes \bar e^{*j}\\
&=\bar x^i_j (\alpha^k_i e_k)\otimes\left((\alpha^{-1})^j_le^{*l}\right)\\
&=\bar x^i_j \alpha^k_i (\alpha^{-1})^j_le_k\otimes e^{*l}.
\end{aligned}
\end{equation*}
For the $x\otimes y$, it comes from the distributive properties of the tensor product.
\end{proof}


\begin{definition}
A \textbf{tensor algebra} $T(V)$ of a vector space $V$ is a set of direct sum
\index{Tensor!Tensor Algebra}
\begin{equation}
T(V)=\bigoplus_{r\ge 0}T^r(V),
\end{equation}
where $T^r(V)=V^r$, a $(r,0)$-type tensor space. With the multiplication is defined as a tensor product. In the similar way, we can define $T(V^*)$ for $V^*$ as well.
\end{definition}

\begin{definition}(Symmetrizatoin and Alternation)
A tensor $x\in T^r(V)$ is said to be \textbf{symmetric} or \textbf{alternative} if 
\begin{equation}
\begin{aligned}
\sigma x &= x,\quad\text{symmetric}\\ 
\sigma x &= \text{sgn}(\sigma)\cdot x,\quad \text{alternative}
\end{aligned}
\end{equation}
where $\mathfrak{S}_r$ is the permutation group of degree $r$ and $\text{sgn}(\sigma)$ is the sign of the permutation $\sigma$ (1 for even permutation and -1 for odd permutation). The permutation action is defined as
\begin{equation}
\begin{aligned}
&\sigma x(v^{*1},\dots,v^{*r})=x\left(v^{*\sigma(1)},\dots,v^{*\sigma(r)}\right),\\
&\sigma x=v_{\sigma^{-1}(1)}\otimes\cdots\otimes v_{\sigma^{-1}(r)},\quad x=v_1\otimes\cdots\otimes v_r.
\label{eq:tensor_permute}
\end{aligned}
\end{equation}
Furthermore, a \textbf{Symmetrizing map} $S_r(x)$ and a \textbf{Alternating map} $A_r(x)$ is defined as
\index{Tensor!Symmetrization and Alternation}
\begin{equation}
\begin{aligned}
S_r(x)&=\frac{1}{r!}\sum_{\sigma\in\mathfrak{S}_r}\sigma x,\\
A_r(x)&=\frac{1}{r!}\sum_{\sigma\in\mathfrak{S}_r}\text{sgn}(\sigma)\sigma x.
\end{aligned}
\end{equation}
We denote $P^r(V),\Lambda^r(V)$ for the set of all symmetric tensor and alternative tensor in $V^r$, repectively.
\end{definition}

\begin{theorem}
Given a $x\in V^r$, it is symmetric or alternative if and only if its coordinate $x^{i_1\dots i_r}$ is symmetric or alternative respect to all indices. For a $\sigma\in \mathfrak{S}_r$, 
\begin{equation}
\sigma[S_r(x)]=S_r(x),\quad \sigma[A_r(x)]=\text{sgn}(\sigma)\cdot A_r(x).
\end{equation}
Furthermore, 
\begin{equation}
P^r(V)=S_r(T^r(V)),\quad \Lambda^r(V)=A_r(T^r(V)).
\end{equation} 
\end{theorem}
\begin{proof}
The first conclusion can be verified directly from the defintion Eq.~\ref{eq:tensor_permute}. The second conclusion comes from a trick that $\tau\sum_{\sigma\in\mathfrak{S}_r}\sigma=\sum_{\sigma\in\mathfrak{S}_r}\sigma,\forall \tau\in\mathfrak{S}_r$, for instance, 
\begin{equation*}
\begin{aligned}
\tau A_r(x)&=\frac{1}{r!}\sum_{\sigma\in\mathfrak{S}_r}\text{sgn}(\sigma)\cdot\tau\circ\sigma(x),\\
&=\frac{\text{sgn}(\tau)}{r!}\sum_{\tau\circ\sigma\in\mathfrak{S}_r}\text{sgn}(\tau\circ\sigma)\cdot(\tau\circ\sigma(x))\\
&=\text{sgn}(\tau)A_r(x).
\end{aligned}
\end{equation*}
This implies that $P^r(V)=S_r(P^r(V))$ and $\Lambda^r=A_r(\Lambda^r(V))$.
\end{proof}

\subsection{Exterior Algebra}
\begin{definition}
Given a vector space $V$. The $\Lambda^r(V)$ is also called \textbf{exterior space} of degree $r$. The element in $\Lambda^r(V)$ is called \textbf{exterior vector}. Suppose $\alpha\in\Lambda^r(V)$ and $\beta\in\Lambda^s(V)$, a \textbf{exterior (wedge) product} is a mapping $\wedge:\Lambda^r(V)\times\Lambda^s(V)\to\Lambda^{r+s}(V)$ defined as
\begin{equation}
\alpha\wedge\beta = A_{r+s}(\alpha\otimes\beta),
\end{equation} 
where $A_{r+s}(x)$ is a alternative mapping.
\index{Exterior Algebra!Exterior vector}
\end{definition}

\begin{theorem}
Suppose $\eta,\eta_1,\eta_2\in\Lambda^k(V)$, $\xi,\xi_1,\xi_2\in\Lambda^l(V)$, and $\gamma\in\Lambda^m(V)$. The wedge product satisfies the following properties
\begin{enumerate}
\item Anticommunity $\eta\wedge\xi =(-1)^{kl}\xi\wedge\eta$;
\item Distribution law $(\eta_1+\eta_2)\wedge\xi=\eta_1\wedge\xi+\eta_2\wedge\xi$;
\item Associative law $(\eta\wedge\xi)\wedge\gamma=\eta\wedge(\xi\wedge\gamma)$.
\end{enumerate}
\end{theorem}

\begin{proof}
\begin{enumerate}
\item Suppose $\tau$ is a permuation swapping $\eta\otimes\xi$ to $\xi\otimes\eta$, since it require $kl$ times exchange then $\text{sgn}(\tau)=(-1)^{kl}$, and 
\begin{equation*}
\begin{aligned}
&\eta\wedge\xi(v^{*1},\dots,v^{*k+l})\\
=&(-1)^{kl}\eta\wedge\xi\left(v^{*\tau(1)},\dots,v^{*\tau(k+l)}\right),\\
=&\frac{(-1)^{kl}}{(k+l)!}\sum_{\sigma\in\mathfrak{S}_{r+s}}\eta(v^{*\sigma\circ\tau(1)},\dots,v^{*\sigma\circ\tau(k)})\cdot\\
&\xi(v^{*\sigma\circ\tau(k+1)},\dots,v^{*\sigma\circ\tau(k+l)})\\
=&\frac{(-1)^{kl}}{(k+l)!}\sum_{\sigma\in\mathfrak{S}_{r+s}}\eta(v^{*\sigma(k+1)},\dots,v^{*\sigma\circ\tau(k+l)})\cdot\\
&\xi(v^{*\sigma\circ\tau(1)},\dots,v^{*\sigma\circ\tau(k)})\\
=&\xi\wedge\eta(v^{*1},\dots,v^{*k+l}).
\end{aligned}
\end{equation*}
\item This comes from the linearity for both alternative mapping and the tensor product.
\item It can be verified by direct calculation. 
\end{enumerate}
\end{proof}

\begin{definition}
A \textbf{exterior algebra} or \textbf{Grassman algebra} is a set $\Lambda(V)$
\begin{equation}
\Lambda(V)=\bigoplus_{r=0}^n\Lambda^r(V),\quad n=\dim V,
\end{equation}
with the addition inherited from the tensor addition and the multiplication is the wedge product. Similarly, $\Lambda(V^*)$ can be defined for $V^*$ where $\Lambda^r(V^*)$ is called the \textbf{exterior form} with degree of $r$. 
\end{definition}

\begin{theorem}
The dimension of $\Lambda(V)$ is $2^n$. The \textbf{evaluation formula} is 
\begin{equation}
\begin{aligned}
&e_{i_1}\wedge\dots\wedge e_{i_r}(v^{*1},\dots,v^{*r})\\
&=\frac{1}{r!}\sum_{\sigma\in\mathfrak{S}_r}\text{sgn}(\sigma)\langle e_{i_1},v^{*1}\rangle\cdots\langle e_{i_r},v^{*r}\rangle,\\
&=\frac{1}{r!}\det(\langle e_{i_\alpha},v^{*\beta}\rangle),\\
&w_{1}\wedge\dots\wedge w_{r}(v^{*1},\dots,v^{*r})=\frac{1}{r!}\det(\langle w_{\alpha},v^{*\beta}\rangle).
\end{aligned}
\end{equation}
Futhermore, for two basis $\{v_i\}_{1\le i\le n}$ and $\{w_i\}_{1\le i\le n}$ related by $w_\alpha=T^\beta_\alpha v_\beta$, then
\begin{equation}
w_1\wedge\cdots\wedge w_r= \det T\cdot v_1\wedge\cdots\wedge v_r.
\end{equation}
Suppose $f:V\to W$ is a linear transform, then
\begin{equation}
f^*(\varphi\wedge\psi)=(f^*\varphi)\wedge(f^*\psi),\quad \varphi,\psi\in \Lambda(V^*),
\end{equation}
that is the map $f^*$ commute with the wedge product.
\end{theorem}

\begin{proof}
Since there's only $n=\dim V$ different basis which makes any $\Lambda^{m}(V)=0,m>n$. So for $\Lambda^r(V),r\le n$, there's only
\begin{equation}
\left(
\begin{aligned}
n\\ r
\end{aligned}
\right)=\frac{n!}{r!(n-r)!}
\end{equation}
possible combination of $r$ basis out of $n$ candidates, so in total
\begin{equation}
2^n=\sum_{r=0}^n\left(
\begin{aligned}
n\\ r
\end{aligned}
\right).
\end{equation}
The evaluation formula is obviously considering the definition of the determinant. To verify the last equation, it is straight forward by applying the following equation
\begin{equation}
f^*\varphi(v_1,\dots,v_n)=\varphi(f(v_1),\dots,f(v_n))
\end{equation}
to check the $f*(\varphi\wedge\psi)$ and $(f^*\varphi)\wedge(f^*\psi)$ separately.
\end{proof}

\begin{theorem}
The necessary and sufficient condition for the vectors $v_1,\dots,v_n$ is linear dependent is that
\begin{equation}
v_1\wedge\cdots\wedge v_n=0.
\end{equation}
\end{theorem}

\subsection{Tensor Bundle and Vector Bundle}
\begin{definition}
Given a $n$-dimensional smooth manifold $M$. A \textbf{fiber} $T^r_s(p)$ of $p$ on $M$ is a (r,s)-type tensor space consisting from the $T_p$ and $T^*_p$, the tangent and cotangent space at $p\in M$. A $(r,s)$-type \textbf{tensor bundle} $T^r_s$ is defined as
\begin{equation}
T^r_s=\bigcup_{p\in M}T^r_s(p),
\end{equation}
Suppose $U$ is a neighborhood on a manifold $M$ with coordinate $(u_1,\dots,u_n)$ and $V^r_s$ is a $(r,s)$-tensor space. A cooridnate chart mapping is defined as
\begin{equation}
\varphi_U:U\times V^r_s\to \bigcup_{p\in U}T^r_s(p),
\end{equation}
This mapping is a isomorphism from $V^r_s$ to $T^r_s(p)$ for any fixed $p\in U$. Furthermore, given a coordinate cover $\{U_1,\dots,U_m\}$ of $M$, and a sequence of mapping $\{\varphi_{U_1},\dots,\varphi_{U_m}\}$. This formed a $C^\infty$-compatible local coordinate system so that a coordinate in chart $\varphi_U(U\times V^r_s)$ is
\begin{equation}
\varphi_U(p,y)=\left(u_i(p),y^{i_1,\dots,i_r}_{j_1,\dots,j_s}\right),\quad y\in V^r_s.
\end{equation}
It implies that $T^r_s$ is a manifold. 
\begin{itemize}
\item A \textbf{bundle projection} is a smooth surjective map $\pi:T^r_s\to M$ mapping all element of $T^r_s(p)$ to $p\in M$.
\item $T^1_0$ is called \textbf{tangent bundle} of $M$ and denoted as $TM$, and $T_1^0$ is called \textbf{cotangent bundle} of $M$, denoted as $T^*M$.
\item A smooth $f:M\to T^r_s$ is called a \textbf{smooth section} or a type $(r,s)$-\textbf{smooth tensor field} if $f(p)\in T^r_s(p),\forall p\in M$, or $\pi\circ f=\text{id}:M\to M$.
\end{itemize}
\end{definition}


To insure the validation of this definition, the $C^\infty$-compatible charts are crutial to make the $T^r_s$ be a manifold. On the other hand, given open covers $U,V$ which $U\cap V\ne \varnothing$, and $y,y'\in V^r_s$, the sufficient and necessary condition for coordinate charts are $C^\infty$-compatible 
\begin{equation}
\varphi_U(p,y)=\varphi_V(p,y'),
\end{equation}
is
\begin{equation}
y'=y\cdot g_{VU}(p),\quad g_{VU}(p):\varphi^{-1}_{V,p}\circ\varphi_{U,p}:V^r_s\to V^r_s,
\end{equation}
where $\varphi_{U,p}(y) = \varphi_{U}(p,y),y\in V^r_s$. The equation above means that $g_{VU}(p)\in \text{GL}(V^r_s)$, an automorphism on $V^r_s$. The following definition is a extension of the construction so far.


\begin{definition}(Vector Bundle)
Given two manifolds $M, E$ and a smooth surjective mapping $\pi:E\to M$ and a $q$-dimensional vector space $V=\realR^q$. If an open cover $\{U_1,\dots\}$ of $M$ and a set of mappings $\{\varphi_{U_1},\dots\}$ satisfy all the following conditions, then $(E,M,\pi)$ is called a (real) $q$-dimensional \textbf{vector bundle} on $M$, where $E$ is called the \textbf{bundle space}, $M$ is called the \textbf{base space}, $\pi$ is called \textbf{bundle projection}, and $V=\realR^q$ is called a \textbf{typical fiber}:
\begin{enumerate}
\item Every map $\varphi_U:U\times \realR^q\to\pi^{-1}(U)$ is a diffeomorphism
\begin{equation}
\pi\circ\varphi_U(p,y)=p,\quad p\in U,y\in\realR^q.
\end{equation}
\item For any fixed $p\in U$,\
\begin{equation}
\varphi_{U,p}(y)=\varphi_U(p,y),\quad y\in\realR^q,
\end{equation}
is a homeomorphism. For any $p\in U\cap W$,
\begin{equation}
g_{UW}(p)=\varphi_{W,p}^{-1}\circ\varphi_{U,p}:\realR^q\to\realR^q
\end{equation}
and $g_{UW}(p)\in \text{GL}(\realR^q)$.
\item The mapping $g_{UW}:U\cap W\to \text{GL}(\realR^q)$ is smooth if $U\cap W\ne\varnothing$.
\end{enumerate}
The space $E_p=\pi^{-1}(p),\forall p\in M$ is called the \textbf{fiber} of vector bundle $E$ at $p$.
This definition can be extended to complex field through substituting the $\text{GL}(\realR^q)$ by $\text{GL}(\complexC^q)$ and $\pi^{-1}(p),p\in M$ will be a $q$-dimensional complex vector space.  A \textbf{smooth section} $s$ is a smooth map $s:M\to E$ preserving
\begin{equation}
\pi\circ E=\text{id}:M\to M.
\end{equation}
The set of all smooth section of a vector bundle $(E,M,\pi)$ is denoted as $\Gamma(E)$.
\end{definition}

\begin{definition}
A map $\varphi_{UW}:U\cap W\to \text{GL}(V)$ is called a \textbf{transition function} of the vector bundle $(E,M,\pi)$ if it satisfies the conditions
\begin{enumerate}
\item For $\forall p\in U$, $g_{UU}=id:U\to U$;
\item If $p\in U\cap W\cap Z\ne\varnothing$, then
\begin{equation}
\varphi_{UW}(p)\cdot\varphi_{WZ}(p)\cdot\varphi_{ZU}(p)=\text{id}:V\to V.
\end{equation}
\end{enumerate}
These two conditions are called the \textbf{compatibility conditions}.
\end{definition}

\begin{theorem}
Suppose $M$ is a $m$ dimensional smooth manifold, $\{U_\alpha\}_{\alpha\in\mathcal{A}}$ is an open covering of $M$, and $V$ is a $q$-dimensional space. If 
\begin{equation*}
\forall \alpha,\beta: U_\alpha\cap U_\beta\ne\varnothing, \exists \varphi_{\alpha\beta}:U_\alpha\cap U_\beta\to \text{GL}(V),
\end{equation*}
satisfies the compatibility conditions. Then there exists a vector bundle $(E,M,\pi)$ with the transition functions $\{\varphi_{\alpha\beta}\}$.
\end{theorem}

\begin{proof}
The idea to prove the existence of such manifold $E$ comes from that a vector bundle can be treated as a manifold from pasting the vector space $V$ into each coordinate of points in $M$. So we start from the pasted manifold 
\begin{equation*}
\tilde{E}=\bigcup_{\alpha\in\mathcal{A}}\{\alpha\}\times U_\alpha\times V,
\end{equation*} 
and shrink the equivalent points, which defined by the transition functions, together. This equivalence $\sim$ for any two points $(\alpha,p,y)$ and $(\beta,p',y')$ in $\tilde E$ is defined as
\begin{enumerate}
\item $p=p',\forall p,p'\in U_\alpha\cap U_\beta$
\item $y=y'\cdot g_{\alpha\beta}(p)$.
\end{enumerate}
The corresponding quotient group $E=\tilde E/~$ is a smooth manifold and the projection $\pi$ is defined as $\pi(\alpha,p,y)=p$. Then $(E,M,\pi)$ formed the vector bundle we need.
\end{proof}



# The brain

The success of the artificial intelligence illustrated a way to us on how to build a simplified neural net from scratch. And how it could be impose the intelligence without extra logistic input from us. The work flow can be classified into three stages: first, build up an neural net architecture; second, setup a bunch of the data with proper labeling for training along with a measure to quantify the performance on the specific inputs; Finally, training the net by using the training samples and test it on other samples to fine tunning the nets, then repeat these steps until the performance we want. In theory, a net could be shaped into any logistic function just like the logic circuit. Although in practice, it might not easy to archive the goal we setup, but these procedures actually could give us incredibly accurate comparing to the designing a program start from the logics. The more we study the artificial neural net we build, the more scare I feel. Not only because I'm afraid that it could reach to equal or higher intelligence level than us, but also the similarity of this idea with our brain. 

When I tried to interpreter our brain, our intelligence base on the view point from the artificial neural net, I found it provides a quite consistent explanation for our mind. Suppose the brain is one kind of pre-trained neural net, we spend our life to train it. Actually, it is more important for a person to train his brain while he was child. According to the research, the brain development of a human could be classified into several stages: For infant, the brain cell will increase dramatically in few months after he/she was born. And this status could be years and the baby will sleep most time in that period. Also, one significant feature for a baby is that he/she has very limited control to their body, even if they tried. That is not only because their brain haven't been trained yet. But also because their brain contained too much cells that they don't need. And this feature has been well understood in the artificial neural net work which is called redundant degree of freedom. The neurons are too much that interference each other, and suppressed the actually useful signals. Too much cells can also leads to over-fitting that creating un-necessary patterns. But training the neural is not as easy as we do in computing simulation. The neuron connections need to be connected and disconnected during the training, and this needs to be carefully processed in real neural system. Not only building a new connection will take time and consuming energy and proteins, but also modifying the neural connection may fundamentally change the functioning of the system. So these operations will be properate to be done while the system is un-functioning. This corresponds to the sleep in animals. And also the test for the neural system may cause the dream. Abstractly speaking, brain needs to propagate the knowledge people learned while they awake to the neural connections of the brain itself, the training and testing steps have been done iteratively. And this leads to our dream circle. 

As mentioned before, a infant will needs to learn and propagate the knowledge to their brain and the cells of their brain is actually double or even tripled in the first few years in their life. The over fitting and redundant degree of freedom may exists in this case. Magically, the studies also show an incredible period that the number of brain cell drastically drop following the booming period. The fascinating feature of this period is that our babies are actually learned how to move adequately, how to response to the world as we expected, and speaking. A typical anti-correlation of the intelligence to the number of brain cell is the key feature of this period. Although it might be confusing at the first glance, but it is natural consequence of the training as we mentioned before, removing and re-normalizing the over fitting. One other remarkable feature of this period is that one giant brain cell actually appeared in our brain. It has huge size comparing to our neural and the connections look more robust as well. Those cells carried significant role in the whole neural system, as they may play the leading role of "weight", the term used in the artificial neural net work to represents the connection parameters. Strong connection may not means the higher "weight" but definitely means that the connection is constantly used in their daily life, which is an other way of "importance".

After that two drastically changing period, our brains finally comes to stable status. The number of the cell will be stable around some certain number, but we are still keeping learning and forgetting in our life. Actually we enter this stage when we are very young and most of our lifetime will stay in this period. It is the time we actually fine tuning our brain. Learning something will slightly change our brain connections and these changes may destroy the information carried in that connection and caused the forgotten. So far, it looks like very thing is reasonable based on this view, or model.

But we could try even more to see how much benefit we could get from this theory. And the first fundamental question is that what our measure is for training our brain. The loss function is a function to measure how much distance the neural net output far away from the truth we expected. Without a loss function, the neural will lost the direction of evolution. Which connection should be removed, which one should be added? These questions are supposed to be answered by minimizing the loss function. But what is the actual loss function for a human? Or we can translate this question into a more philosophy way: what is the meaning of our life?

It is very interesting to try to answer this question. Let's first think of the data we used for training. The data actually comes from our life, and in principle, a label is needed for us to anchor our measure to the real world. How do we actually know that how much close we get from our goal? Well, the label can either hidden in the environment we interacting, or comes from our brain itself. The former case is easy to understand when we think a small goal in our life. Saying, you are going to drink water. The desire of drinking water comes from our body requirement and it is burn in our life if it was functioning well. So we have very basic needs from our body to provide initial motivation to do something. When those needs are satisfied, we will remember how we manage to release ourselves from that desire. And consequently we learned the correlation between the water and thirsty. Suppose we knew that grab a cup of water and drink is a way to release the thirsty, we have another goal raising from the initial motivation. And how to grab a cup is our next goal. The loss function will be more important for us to train ourselves to control our bodies. 

Unlike the neural net work we made in the code, human has variety ways to interacting with the environment. It is technically called I/O for artificial intelligence, which haven't been studied extensively yet. We measure the distance between our hand and cup by using not only the vision, but also the feeling of our skin. We move our arm to touch the cup until we feel it through our skin. We might already forgotten how clumsy we were when we first trying to grab something. But just look at baby you will know what that process looks like. We were correcting our hand for direction and adjusting the muscle constantly to control the moving speed and distance. After we mastered this skill, very related moving behaviors as natural as it was. And this seems answered the first part of the question. How we actually setup the loss function for ourselves. 

# Id

\section{Distributions}

\subsection{Poisson Distribution}
\begin{definition}[Poisson assumption]
Assume a integer valued random variable $K$ with a PDF $g(k,h)$ where an parameter $h$ satisfying the following assumption when $h\to 0$:
\begin{enumerate}
\item $g(1,h) = \lambda h+o(h)$;
\item $\sum_{k=2}^{\infty}g(k,h)=o(h)$;
\item $g(0,h)g(0,w)=g(0,h+w)$;\label{eq:poisson_hpyer3}
\item $g(x,w+h)=g(x,w)g(0,h)+g(x-1,w)g(1,h)$.\label{eq:poisson_hpyer4}
\end{enumerate}
Then $g(x,w)$ is a \textbf{Poisson distribution}:
\begin{equation}
g(x,w)=\frac{1}{x!}(\mu)^xe^{-\mu},\quad \mu=\lambda w\,\, \text{and}\,\, x=0,1,2,\dots
\end{equation}
\end{definition}
\begin{proof}
The $o(h)$ means that $\lim_{h\to 0}o(h)/h=0$. 
\begin{equation*}
\begin{aligned}
g(0,w+h)&=g(0,w)[1-\lambda h-o(h)],\\
\frac{dg(0,w)}{dw}&=\lambda g(0,w),\\
g(0,w)&=ce^{-\lambda w}.
\end{aligned}
\end{equation*}
Repeat the similar procedure to Eq.\ref{eq:poisson_hpyer4}, we get the formula:
\begin{equation*}
\partial_wg(x,w)=-\lambda g(x,w)+\lambda g(x-1,w).
\end{equation*}
Using this equation, the conclusion can be approved by induction.
\end{proof}
Suppose $g(x,h)$ is the probability of $x$ changes in a interval with a width $h$, if it satisfies the Poisson's assumptions, it means that the event that changing of $x$ depends only on the width of the interval and this probability can be approximated linearly when $h$ is small enough. One example of many applications satisfying the Poisson's assumptions is that the atomic decay with time. In this case, $g(x,h)$ represents the number of decays $x$ happened inside of time interval $h$. 

\begin{theorem}
Suppose $X\sim\poisson(\mu)$, then
\begin{enumerate}
\item $\expect(X)=\mu$ and $\var(X)=\mu^2$;
\item The moment generating function
\begin{equation}
M_X(t)=\exp[\mu(e^t-1)];
\end{equation}
\item If $Y=\sum_iX_i$ where $X_i\sim\poisson(\mu_i)$ and $x_i\ci x_j,\forall i\ne j$, then
\begin{equation}
Y\sim\poisson\left(\sum_i\mu_i\right).
\end{equation}
\end{enumerate}
\end{theorem}

\begin{proof}
We first prove the second point and the first one follows from the theorem~\ref{th:mgf}. In fact
\begin{equation*}
\expect(e^{x t})=\sum_{x=0}^\infty e^{x t}\frac{\mu^x e^{-\mu t}}{x!}=\sum_{x=0}^\infty\frac{(\mu e^t)^xe^{-\mu}}{x!}=e^{\mu(e^t-1)}.
\end{equation*} 
\end{proof}



\subsection{Normal distributions}
\begin{definition}
\index{Normal Distributions}
A \textbf{normal distribution} with mean $\mu$ and variance $\sigma^2$, denoted as $N(\mu,\sigma^2)$ is 
\begin{equation}
N(\mu,\sigma^2)(x) = \frac{1}{\sqrt{2\pi}\sigma^2}\exp\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\}.
\end{equation}
If a random variable $X\sim N(\mu,\sigma^2)$, this variable is called \textbf{Gaussian}. And we call $X$ as \textbf{standard normal} variable if $X\sim N(0,1)$.
A random varialbe $Y$ is called n-dimensional Gassian random variable if $\boldsymbol{X}=(X_1,X_2,...,X_n)$, where $X_i\sim N(\mu_i,\sigma_i^2)$ and $X_i\ci X_j,\forall i\ne j$.
\end{definition}

\begin{theorem} 
Assume $X_i\sim N(\mu_i,\sigma_i^2)$, $X_i\ci X_j,\forall i\ne j$, $a_i\in\realR$, and $Y=\sum_{i=1}^na_iX_i$, then:
	\begin{enumerate}
		\item The moment generating function of $X$ is $M_X(t)=\exp(\mu t+\sigma^2t^2/2)$
		\item The PDF of $Y$ is
			\begin{equation}
			\begin{aligned}
			Y &\sim N\left(\sum_{i=1}^na_i\mu_i,\sum_{i=1}^na^2_i\sigma_i^2\right).
			\end{aligned}
			\end{equation}
		\item If $Z_i=(X_i-\mu_i)/\sigma_i$, then $Z_i\sim N(0,1)$;
		\item $Z^2_i\sim \chi^2(1)$, and if $Y=\sum_{i=1}^nZ_i^2$, then $Y\sim \chi^2(n)$.
	\end{enumerate}
\end{theorem}
\begin{proof} Just briefly draw the line of the proof:
	\begin{enumerate}
		\item It follows from the straight forward calculation of $\mathbb{E}(e^{Xt})$.
		\item Consider $n=2$ case, since $X_i$ are independent, the moment generating function is
		\begin{equation*}
		\begin{aligned}
		M_{a_1X_1+a_2X_2}(t)&=M_{a_1X_1}(t)M_{a_2X_2}(t)\\
		&=\exp\left[a_1\mu_1+a_2\mu_2+\frac{(a_1^2\sigma_1^2+a_2^2\sigma_2^2)t^2}{2}\right],
		\end{aligned}
		\end{equation*}
		which is the same as the variable $Z\sim N(a_1\mu_1+a_2\mu_2,a_1^2\sigma_1+a_2^2\sigma_2)$. On the other hand, if we consider the PDF $f(cx)dx$ with substituting the $x$ by $y=x/c$, the calculation leads to $g(y)dy$ where $g(y)=N(c\mu,c^2\sigma^2)$. It implies that $M_{a_iX_i}(t)=\exp(a_i\mu_i+a_i^2\sigma^2_i/2)$.
		\item Here we need to show that $X-c\sim~N(\mu-c,\sigma^2)$ if $X\sim N(\mu,\sigma^2)$. In fact, shifting the integral center by a finite number won't affect the integral as the integral range is $[-\infty,+\infty]$.
		\item We start from calculating $\expect(e^{Z^2t})$ for $Z\sim N(0,1)$, then
		\begin{equation*}
		\begin{aligned}
		\expect(e^{Z^2t})&=\int_{-\infty}^{\infty}e^{z^2t}e^{-z^2/2}\frac{dz}{\sqrt{2\pi}},\\
		&=\int_0^\infty e^{xt}\frac{x^{-\frac{1}{2}}e^{-x/2}}{\sqrt{2\pi}}dx,
		\end{aligned}
		\end{equation*}
		which is the mgf of $\chi^2(1)=\Gamma(1/2,2)$. For the general case, notice that the volume $V_n(R)$ of a $n$-dimensional ball is
		\begin{equation}
		V_n(R)=\frac{\pi^{n/2}R^n}{\Gamma(n/2+1)},
		\label{eq:nball_volume}
		\end{equation}
		so that the mgf of $Y=\sum_iZ_i^2$ is
		\begin{equation}
		\begin{aligned}
		\expect Y &= \prod_{i=1}^n\left\{\int \frac{e^{-z_i^2/2+z^2_it}dz_i}{\sqrt{2\pi}}\right\}\\
		&=\int_0^{\infty}\frac{1}{(2\pi)^{n/2}}e^{-r^2/2+r^2t}\frac{\partial V_n(r)}{\partial r}dr,\\
		&=\int_0^\infty\frac{\rho^{n/2-1}e^{-\rho/2}}{\Gamma(n/2)2^{n/2}}e^{\rho t}d\rho,
		\end{aligned}
		\end{equation}
		where I substituted $R$ by $\rho=R^2$ and the final expression is the mgf of $\Gamma(n/2,2)=\chi^2(n)$.
	\end{enumerate}
\end{proof}


\subsection{\texorpdfstring{$\Gamma$}- and \texorpdfstring{$\chi^2$}-distributions}

\begin{definition}
A Gamma distribution $\Gamma(\alpha,\beta), \alpha,\beta>0$ is defined as
\begin{equation}
\begin{aligned}
\Gamma(\alpha,\beta)(x)&=\frac{x^{\alpha-1}e^{-x/\beta}}{\Gamma(\alpha)\beta^\alpha}\Theta(x),\\
\Gamma(\alpha)&=\int_0^\infty\frac{x^{\alpha-1}e^{-x/\beta}}{\Gamma(\alpha)\beta^\alpha}dx,
\end{aligned}
\end{equation}
where $\Theta(x)$ is a step function $\Theta(x)=1,\forall x\ge0$ and $\Theta(x)=0,\forall x<0$.
\end{definition}

\begin{theorem}
Given a variable $X\sim\Gamma(\alpha,\beta)$, we have 
\begin{enumerate}
\item The mgf $M_X(t)=(1-\beta t)^{-\alpha}$ for $t<\beta^{-1}$;
\item $\expect X=\alpha\beta$ and $\var X =\alpha\beta^2$;
\item Suppose $X_i\sim\Gamma(\alpha_i,\beta)$ and $Y=\sum_iX_i$, then
\begin{equation}
Y\sim\Gamma\left(\sum_{i=1}^n\alpha_i,\beta\right).
\end{equation}
\end{enumerate}
\label{th:gamma_dist_properties}
\end{theorem}

\begin{proof}
To validate the 1st equation
\begin{equation}
\begin{aligned}
\expect e^{Xt}&=\int_0^\infty \frac{x^{\alpha-1}e^{(-x/\beta+xt)}}{\Gamma(\alpha)\beta^\alpha}dx,\\
&=\int_0^\infty\frac{x^{\alpha-1}e^{-x/\beta'}}{\Gamma(\alpha)\beta'^\alpha}dx,\\
&=(1+\beta't)^\alpha\\
&=(1-t\beta)^{-\alpha},\quad t\beta<1,
\end{aligned}
\end{equation}
where $1/\beta'=t+1/\beta>0$. Then the expectation and variance follows immediately. The 3rd point is obvious from checking mgf $M_Y(t)=\prod_iM_{X_i}(t)$.
\end{proof}

\begin{definition}
If $X\sim\Gamma(n/2,2),n \in \mathbb{Z}^+$ (positive integer), we say it is a $\chi^2$-distribution denoted as $X\sim\chi^2(n)$. $\expect X=n$, $\var(X)=2n$ from the theorem~\ref{th:gamma_dist_properties}.
\end{definition}